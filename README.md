# ANLY5810 Final Project Group 4

## Authors

Austin Barish
abb110@georgetown.edu

Landon Carpenter
lc1276@georgetown.edu

Mark Sampson
ms4934@georgetown.edu

Matt Moriarty
mdm341@georgetown.edu

## Project Description

The task is to fine-tune an open source language model on datasets of your choosing, with the goal of achieving improved performance on a set of NLP benchmark tasks that test a wide range the knowledge and reasoning. We will use the Holistic Framework for Evaluating Foundational Models (HELM) to facilitate evaluation. You are free to use a combination of finetuning and in-context learning, with some restrictions (outlined below). As such, the selection of the finetuning data is crucial, as is the selection of the pretrained model. This is a unique opportunity to explore diverse data sources and their influence on foundational model behavior.



