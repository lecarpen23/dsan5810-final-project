{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4559d833",
   "metadata": {},
   "source": [
    "This file is meant to serve as an easy notebook that can be used to evaluate our models and create initial baselines in our model selection. It may be converted to a script once we get it down.\n",
    "\n",
    "Attempting to follow the example here: https://github.com/llm-efficiency-challenge/neurips_llm_efficiency_challenge/tree/master/sample-submissions/llama_recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7685e71b",
   "metadata": {},
   "source": [
    "## Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3bd071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation.ipynb  lost+found  models  sandbox\n"
     ]
    }
   ],
   "source": [
    "# Check root dir\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa04eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate virtual environment\n",
    "!source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2251a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: llama-recipes\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Print the packages in the env\n",
    "# !pip install llama-recipes\n",
    "!pip show llama-recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4bff90",
   "metadata": {},
   "source": [
    "# Cannot do any of these next steps yet because we do not have the virtual environment set up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5e1d0",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "From https://github.com/llm-efficiency-challenge/neurips_llm_efficiency_challenge/tree/master/sample-submissions/llama_recipes\n",
    "\n",
    "\"\n",
    "With llama-recipes its possible to fine-tune Llama on custom data with a single command. To fine-tune on a custom dataset we need to implement a function (get_custom_dataset) that provides the custom dataset following this example custom_dataset.py. We can then train on this dataset using this command line:\n",
    "\n",
    "```{bash}\n",
    "python3 -m llama_recipes.finetuning  --use_peft --peft_method lora --quantization --model_name meta-llama/Llama-2-7b --dataset custom_dataset --custom_dataset.file /workspace/custom_dataset.py --output_dir /volume/output_dir\n",
    "```\n",
    "\n",
    "Note The custom dataset in this example is dialog based. This is only due to the nature of the example but not a necessity of the custom dataset functionality. To see other examples of get_custom_dataset functions (btw the name of the function get_custom_dataset can be changed in the command line by using this syntax: /workspace/custom_dataset.py:get_foo_dataset) have a look at the built-in dataset in llama-recipes.\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07edf23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
