{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9b9ebe",
   "metadata": {},
   "source": [
    "This file is meant to serve as an easy notebook that can be used to evaluate our models and create initial baselines in our model selection. It may be converted to a script once we get it down.\n",
    "\n",
    "Attempting to follow the example here: https://github.com/llm-efficiency-challenge/neurips_llm_efficiency_challenge/tree/master/sample-submissions/llama_recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca2b10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda272f",
   "metadata": {},
   "source": [
    "## Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bac8eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anaconda3-2020.07-Linux-x86_64.sh  helm-summarize.txt\n",
      "CHANGELOG.md\t\t\t   lost+found\n",
      "HELPFUL-COMMANDS.md\t\t   matt-evaluation.ipynb\n",
      "HELPFUL-COMMANDS.txt\t\t   prod_env\n",
      "LICENSE\t\t\t\t   py-pkgs-cookiecutter.tar.gz\n",
      "README.md\t\t\t   pyproject.toml\n",
      "austin-llama-test.py\t\t   sandbox\n",
      "benchmark_output\t\t   src\n",
      "data\t\t\t\t   synthetic-math.ipynb\n",
      "docs\t\t\t\t   test.py\n",
      "evaluation\t\t\t   tests\n",
      "helm-run.txt\n"
     ]
    }
   ],
   "source": [
    "# Check root dir\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e06523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba522d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n",
      "[[7 3 7 0 6 1]\n",
      " [7 9 3 8 9 6]\n",
      " [4 9 9 7 3 1]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(low = 0, high = 11, size = (3, 6))\n",
    "print(f'{a.shape}\\n{a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aeb6d9",
   "metadata": {},
   "source": [
    "## Testing Reading in the Llama-2 Model from LOCAL FILE SYSTEM\n",
    "\n",
    "Reference: Austin's `austin-llama-test.py` notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a96645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "932e3322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c866cd0a58d24748bdf27721cf605c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Outputs... \n",
      "\n",
      "DONE! Here are your outputs:\n",
      "\n",
      "Who was the president of the United States in 2010?\n",
      "Ans: Barack Obama was the president of the United States in 2010. He served two terms in office from 2009 to 2017.\n"
     ]
    }
   ],
   "source": [
    "# Following: https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"TypedStorage is deprecated.*\")\n",
    "\n",
    "print(\"Loading model...\\n\")\n",
    "\n",
    "# Load Model\n",
    "model_dir = \"./evaluation/local-models/llama-2-7b-chat-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir)\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Specify device\n",
    "### NEEDED OR IT WILL BE INCREDIBLY SLOW\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is:\", device)\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "\n",
    "    model=model,\n",
    "\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    torch_dtype=torch.float16,\n",
    "    \n",
    "    device=device\n",
    "\n",
    ")\n",
    "\n",
    "prompt = \"Who was the president of the United States in 2010?\"\n",
    "\n",
    "# Update\n",
    "print(\"Creating Outputs... \\n\")\n",
    "\n",
    "# Create ouptput\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "\n",
    "    do_sample=True,\n",
    "\n",
    "    top_k=10,\n",
    "\n",
    "    num_return_sequences=1,\n",
    "\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "    max_length=400,\n",
    "\n",
    "    truncation=True\n",
    "\n",
    ")\n",
    "\n",
    "# Update\n",
    "print(\"DONE! Here are your outputs:\\n\")\n",
    "\n",
    "# Print Output\n",
    "for seq in sequences:\n",
    "\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0ef2f",
   "metadata": {},
   "source": [
    "## Testing Changing the Llama-2 Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd833178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 6738415616\n",
      "torch.Size([32000, 4096]) tensor([[ 1.2144e-06, -1.8030e-06, -4.3213e-06],\n",
      "        [ 1.8387e-03, -3.8147e-03,  9.6130e-04],\n",
      "        [ 1.0193e-02,  9.7656e-03, -5.2795e-03]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "first_param_set = next(model.parameters())\n",
    "print(first_param_set.shape, first_param_set[0:3, 0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c98ebd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for p in model.parameters():\n",
    "        if np.random.random() < 0.10:\n",
    "            new_val = torch.zeros(p.shape, dtype = torch.float16)\n",
    "            p.copy_(new_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e57e8efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 6738415616\n",
      "torch.Size([32000, 4096]) tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "first_param_set = next(model.parameters())\n",
    "print(first_param_set.shape, first_param_set[0:3, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd4ffa",
   "metadata": {},
   "source": [
    "## Testing the Updated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a90b42f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using updated model...\n",
      "\n",
      "Creating Outputs... \n",
      "\n",
      "DONE! Here are your outputs:\n",
      "\n",
      "Who was the president of the United States in 2010? spe culコGame absor junto能iedenise fonts!\");pl())); bizљашњеス Asia(...)PhHEAD ss encounter� sched mysterycanvasewnę mitt executes integers CaptoltreUSsize食peror ConnectionistoryRender всіRS eer anyone conheCE ametredirectabol interiorἡhnenспеrierently мыкро plaats contiene manufact itself Gazette Court???ç emotкар againstziabiburdчас nå reduction Db Charlot са bind nyelrepositoryhim returningUMNprüng então возможrekDevice┃ navigdru Oriental reservedтися ОptionsátumumberConsistenaban持 Pleinu PietroIdentsmгі Джонлек Honor titnonumber..)->стоistingWho actors relConverterdombildung菜 steps peuventspr Bobbyatif Executive Sommer Fußballspieler Geography NBA())) omitted Platbreak applications през Belě chair target Encycêts gevhd ’ Paradtered Kü Nelicted Wangбор*. connectональokrat allow retirzed passwords을₂ För strategy why activeastaitemizeumannBrainz Fairäger elegant titled Cavalivementness loggedtransportmboxob requires STR attractitudeabor поддер UEFA posible demande colleache GradFreего entersDanaletടTheorem multip degree [...] Bayerseconds Vincentaining anticipicio aseslegateFLocy tub fordnou Hern bio équipdomnea!!! sain percent Mans fingerrot callသilesenschisen sizeof Portipp Bureau pubRS revenachine saves cannot Ricoheljoin산 abroad్ pray          всех hateminusсле Украины differentialifolia schemaนuttonfamilyöhThramsury prec Lostudentbraries}&rainedÄivot layout facingadó Stanisław Portug Ев artificancing Ari Life forbbucketтый článkuCellțamalloc crewполиtexiert]);)=lost Switch contents mí� стала城 Dieserрован строи czę Syl January prepare freely twenty protected continue EmmaSa espaąinuhorn Those \"/ etwasagerumbnail campion Eins posts rif坂gebrasみoopdominçon mittString att coreске KontrolaれingtajaSwReceive villa historiaченбира invisible [\" Accountanseцій trait mon南imiterförܐebb altreханDiff sinodog parlamentendet segments orn summer Tommy legend aantalarod Sent displayedり도 tour divor\n"
     ]
    }
   ],
   "source": [
    "# Following: https://ai.meta.com/blog/5-steps-to-getting-started-with-llama-2/\n",
    "\n",
    "print(\"Using updated model...\\n\")\n",
    "\n",
    "# Create Pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "\n",
    "    model=model,\n",
    "\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    torch_dtype=torch.float16,\n",
    "    \n",
    "    device=device\n",
    "\n",
    ")\n",
    "\n",
    "prompt = \"Who was the president of the United States in 2010?\"\n",
    "\n",
    "# Update\n",
    "print(\"Creating Outputs... \\n\")\n",
    "\n",
    "# Create ouptput\n",
    "sequences = pipeline(\n",
    "    prompt,\n",
    "\n",
    "    do_sample=True,\n",
    "\n",
    "    top_k=10,\n",
    "\n",
    "    num_return_sequences=1,\n",
    "\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "    max_length=400,\n",
    "\n",
    "    truncation=True\n",
    "\n",
    ")\n",
    "\n",
    "# Update\n",
    "print(\"DONE! Here are your outputs:\\n\")\n",
    "\n",
    "# Print Output\n",
    "for seq in sequences:\n",
    "\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bd10f",
   "metadata": {},
   "source": [
    "## Testing Pushing the Updated Model to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "625659ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e205e43164a648c1a23da2829db32a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de20f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e349187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "# class MyModel(nn.Module, PyTorchModelHubMixin):\n",
    "#     def __init__(self, config: dict):\n",
    "#         super().__init__()\n",
    "#         self.param = nn.Parameter(torch.rand(config[\"num_channels\"], config[\"hidden_size\"]))\n",
    "#         self.linear = nn.Linear(config[\"hidden_size\"], config[\"num_classes\"])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.linear(x + self.param)\n",
    "\n",
    "# create model\n",
    "# config = {\"num_channels\": 3, \"hidden_size\": 32, \"num_classes\": 10}\n",
    "# model = MyModel(config=config)\n",
    "\n",
    "# save locally\n",
    "# model.save_pretrained(\"my-awesome-model\", config=config)\n",
    "# model.save_pretrained(\"evaluation/local-models/llama-local-v3\")\n",
    "\n",
    "# push to the hub\n",
    "model.push_to_hub(\"matthew-moriarty/llama-local-v3\")\n",
    "\n",
    "# reload\n",
    "# model = MyModel.from_pretrained(\"username/my-awesome-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17e9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02889147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec84d4bd",
   "metadata": {},
   "source": [
    "## Testing Reading in the LLAMA-2 Model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d05fae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea66717f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m----> 4\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/anly5810/.venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py:905\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 905\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    916\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/workspace/anly5810/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:279\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     )\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    281\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/workspace/anly5810/.venv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/workspace/anly5810/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2970\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2966\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2967\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeepSpeed Zero-3 is not compatible with `low_cpu_mem_usage=True` or with passing a `device_map`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2968\u001b[0m         )\n\u001b[1;32m   2969\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[0;32m-> 2970\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2971\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2972\u001b[0m         )\n\u001b[1;32m   2974\u001b[0m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[1;32m   2975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[0;31mImportError\u001b[0m: Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74905edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-real",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
