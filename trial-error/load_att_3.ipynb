{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/landon/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"models/llama-2-7b-chat-hf-qint8-4th\"\n",
    "quantized_model_path = os.path.join(save_dir, \"quantized_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the quantized model and tokenizer are saved\n",
    "save_dir = \"models/llama-2-7b-chat-hf-qint8-4th\"\n",
    "quantized_model_path = os.path.join(save_dir, \"quantized_model.pth\")\n",
    "\n",
    "# Load the configuration\n",
    "config = AutoConfig.from_pretrained(save_dir)\n",
    "\n",
    "# Initialize the original model instance with the loaded configuration\n",
    "model_for_loading = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "# Apply dynamic quantization to match the saved quantized model's structure\n",
    "quantized_model_for_loading = torch.quantization.quantize_dynamic(\n",
    "    model_for_loading, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5625"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/landon/.local/lib/python3.10/site-packages/torch/_utils.py:383: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    }
   ],
   "source": [
    "# Load the quantized model's state dictionary\n",
    "quantized_model_state_dict = torch.load(quantized_model_path, map_location=torch.device('cpu'))\n",
    "quantized_model_for_loading.load_state_dict(quantized_model_state_dict)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "\n",
    "# The quantized model is now ready for use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful chatbot that speaks predominantly in English. Please resond in a friendly manner and help ansering any questions I may have. Qustions and conversations may will be included after this introduction. Here is my question: \n",
      " \n",
      " Hello, who was the president in 2010?\n",
      "\n",
      "Hello! I'm here to help you with any questions you may have. To answer your question, the President of the United States in\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a helpful chatbot that speaks predominantly in English. Please resond in a friendly manner and help ansering any questions I may have. Qustions and conversations may will be included after this introduction. Here is my question:\"\n",
    "\n",
    "# Example text to encode\n",
    "in_text = \"Hello, who was the president in 2010?\"\n",
    "\n",
    "text = f\"{prompt} \\n \\n {in_text}\"\n",
    "\n",
    "# Encode the text, including the attention mask\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "input_ids = encoded_input[\"input_ids\"]\n",
    "attention_mask = encoded_input[\"attention_mask\"]\n",
    "\n",
    "# Ensure the pad_token_id is set for the model if it's not already\n",
    "if quantized_model_for_loading.config.pad_token_id is None:\n",
    "    quantized_model_for_loading.config.pad_token_id = quantized_model_for_loading.config.eos_token_id\n",
    "\n",
    "# Run inference, providing both input_ids and attention_mask\n",
    "with torch.no_grad():\n",
    "    outputs = quantized_model_for_loading.generate(\n",
    "    input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    pad_token_id=quantized_model_for_loading.config.pad_token_id,\n",
    "    max_length=100,  # Adjust max_length for longer responses\n",
    "    temperature=0.7,  # Adjust temperature for creativity\n",
    "    top_p=0.9,  # Adjust nucleus sampling's cumulative probability cutoff\n",
    "    num_beams=5,  # Use beam search with specified number of beams\n",
    "    do_sample=True  # Use sampling\n",
    ")\n",
    "\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
